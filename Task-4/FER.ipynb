{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bea9dc",
   "metadata": {},
   "source": [
    "FER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc6a748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "763477f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"FER\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "assert os.path.exists(TRAIN_DIR), f\"Training directory not found: {TRAIN_DIR}\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a01e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),     # convert 1-channel â†’ 3-channel\n",
    "    transforms.RandomHorizontalFlip(),                                   \n",
    "    transforms.Resize((224, 224)),                    # ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],                   # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fc85183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 22967\n",
      "Val samples  : 5742\n"
     ]
    }
   ],
   "source": [
    "full_dataset = ImageFolder(TRAIN_DIR, transform=train_transform)\n",
    "\n",
    "train_len = int(0.8 * len(full_dataset))\n",
    "val_len = len(full_dataset) - train_len\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_len, val_len]\n",
    ")\n",
    "\n",
    "val_dataset.dataset.transform = eval_transform\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples  :\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8518b643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_classes):\n",
    "\n",
    "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    \"\"\"ONLY FC TRAINING\n",
    "    # FREEZE ENTIRE BACKBONE\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\"\"\"\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \"\"\"FOR PARTIAL TRAINING (ONLY OF LAYER 4)\n",
    "    # Unfreeze Only the last ResNet block (layer4)\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\"\"\"\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Trainable parameters:\")\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7696db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22538ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "040199b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images).argmax(dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return acc, p, r, f1, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af7988cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 1.1606 | Val Acc: 0.6146 | Val F1: 0.5522\n",
      "Epoch 2/15 | Train Loss: 0.7820 | Val Acc: 0.6402 | Val F1: 0.5917\n",
      "Epoch 3/15 | Train Loss: 0.4615 | Val Acc: 0.6426 | Val F1: 0.6073\n",
      "Epoch 4/15 | Train Loss: 0.1876 | Val Acc: 0.6289 | Val F1: 0.6067\n",
      "Epoch 5/15 | Train Loss: 0.0723 | Val Acc: 0.6479 | Val F1: 0.6237\n",
      "Epoch 6/15 | Train Loss: 0.0421 | Val Acc: 0.6493 | Val F1: 0.6316\n",
      "Epoch 7/15 | Train Loss: 0.0373 | Val Acc: 0.6304 | Val F1: 0.6111\n",
      "Epoch 8/15 | Train Loss: 0.0908 | Val Acc: 0.6203 | Val F1: 0.5800\n",
      "Epoch 9/15 | Train Loss: 0.1329 | Val Acc: 0.6529 | Val F1: 0.6306\n",
      "Epoch 10/15 | Train Loss: 0.0556 | Val Acc: 0.6374 | Val F1: 0.6209\n",
      "Epoch 11/15 | Train Loss: 0.0282 | Val Acc: 0.6458 | Val F1: 0.6264\n",
      "Epoch 12/15 | Train Loss: 0.0196 | Val Acc: 0.6607 | Val F1: 0.6444\n",
      "Epoch 13/15 | Train Loss: 0.0146 | Val Acc: 0.6526 | Val F1: 0.6379\n",
      "Epoch 14/15 | Train Loss: 0.0338 | Val Acc: 0.6142 | Val F1: 0.5973\n",
      "Epoch 15/15 | Train Loss: 0.1611 | Val Acc: 0.6392 | Val F1: 0.6225\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "    val_acc, val_p, val_r, val_f1, _ = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f} | \"\n",
    "        f\"Val F1: {val_f1:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fee5d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FER2013 (Frozen Backbone) Results\n",
      "Accuracy : 0.6391501219087427\n",
      "Precision: 0.6284307035426985\n",
      "Recall   : 0.6222899748405688\n",
      "F1-score : 0.6225062441760414\n",
      "Confusion Matrix:\n",
      " [[ 461    5   82   43   84   89   35]\n",
      " [  14   44    5    0    3    7    3]\n",
      " [ 102    7  356   35   78  136  106]\n",
      " [  48    1   27 1180  119   28   42]\n",
      " [  77    1   60   48  678  137   24]\n",
      " [ 123    3  144   29  188  470   26]\n",
      " [  20    2   31   34   15   11  481]]\n"
     ]
    }
   ],
   "source": [
    "val_acc, val_p, val_r, val_f1, val_cm = evaluate(model, val_loader)\n",
    "\n",
    "print(\"\\nFER2013 (Frozen Backbone) Results\")\n",
    "print(\"Accuracy :\", val_acc)\n",
    "print(\"Precision:\", val_p)\n",
    "print(\"Recall   :\", val_r)\n",
    "print(\"F1-score :\", val_f1)\n",
    "print(\"Confusion Matrix:\\n\", val_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414808c",
   "metadata": {},
   "source": [
    "on training only the fc layer  \n",
    "  \n",
    "Epoch 1/15 | Train Loss: 1.6381 | Val Acc: 0.4110 | Val F1: 0.3223  \n",
    "Epoch 15/15 | Train Loss: 1.4044 | Val Acc: 0.4424 | Val F1: 0.3805  \n",
    "\n",
    "  \n",
    "FER2013 (Frozen Backbone) Results  \n",
    "Accuracy : 0.44235458028561475  \n",
    "Precision: 0.3928199665163275  \n",
    "Recall   : 0.39119934995031885  \n",
    "F1-score : 0.3805418303351867  \n",
    "Confusion Matrix:  \n",
    " [[192   3  57 189 110 152  75]  \n",
    " [  9  15   4  21   8  18   8]  \n",
    " [ 83  13 134 156 115 174 163]  \n",
    " [ 61  10  36 997 138 122  86]  \n",
    " [ 60   7  42 223 395 143  76]  \n",
    " [ 97  10  64 237 159 351  68]  \n",
    " [ 30   1  45  68  37  24 456]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2f79c",
   "metadata": {},
   "source": [
    "on partial training (layer 4 of resnet with fc layer)  \n",
    "  \n",
    "Epoch 1/15 | Train Loss: 1.2469 | Val Acc: 0.5817 | Val F1: 0.4948  \n",
    "Epoch 15/15 | Train Loss: 0.0111 | Val Acc: 0.6109 | Val F1: 0.5852  \n",
    "  \n",
    "FER2013 (Frozen Backbone) Results  \n",
    "Accuracy : 0.610936955764542  \n",
    "Precision: 0.6136706991647694  \n",
    "Recall   : 0.573162193188752  \n",
    "F1-score : 0.5852134488354735  \n",
    "Confusion Matrix:  \n",
    " [[ 410    9   62   65  126  114   30]  \n",
    " [  16   36    6    8    6   11    2]  \n",
    " [ 120    3  338   43  101  138   94]  \n",
    " [  45    0   15 1165  102   59   34]  \n",
    " [  66    0   43   97  596  167   29]  \n",
    " [  97    2   93   60  189  485   28]  \n",
    " [  30    0   39   25   36   24  478]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0de7f5",
   "metadata": {},
   "source": [
    "on full - fine tuning  \n",
    "  \n",
    "Epoch 1/15 | Train Loss: 1.1606 | Val Acc: 0.6146 | Val F1: 0.5522  \n",
    "Epoch 15/15 | Train Loss: 0.1611 | Val Acc: 0.6392 | Val F1: 0.6225  \n",
    "  \n",
    "    \n",
    "FER2013 (Frozen Backbone) Results  \n",
    "Accuracy : 0.6391501219087427  \n",
    "Precision: 0.6284307035426985  \n",
    "Recall   : 0.6222899748405688  \n",
    "F1-score : 0.6225062441760414  \n",
    "Confusion Matrix:  \n",
    " [[ 461    5   82   43   84   89   35]  \n",
    " [  14   44    5    0    3    7    3]  \n",
    " [ 102    7  356   35   78  136  106]  \n",
    " [  48    1   27 1180  119   28   42]  \n",
    " [  77    1   60   48  678  137   24]  \n",
    " [ 123    3  144   29  188  470   26]  \n",
    " [  20    2   31   34   15   11  481]]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptonite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
