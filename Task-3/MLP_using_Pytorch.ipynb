{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b629a6c",
   "metadata": {},
   "source": [
    "MLP using Pytorch on Adult Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cf2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b38138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult_dataset(csv_path):\n",
    "    column_names = [\n",
    "        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "        \"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
    "        \"native-country\", \"income\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_csv(csv_path, names=column_names, na_values=\" ?\")\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert target to binary\n",
    "    df[\"income\"] = (df[\"income\"].str.strip() == \">50K\").astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2360c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Splits features into numerical and categorical,\n",
    "    scales numerical features, one-hot encodes categorical features.\n",
    "    \"\"\"\n",
    "    target = df[\"income\"].values\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=[\"int64\"]).columns.drop(\"income\")\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_num = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    X_cat = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Combine features\n",
    "    X = np.hstack([X_num, X_cat]).astype(np.float32)\n",
    "\n",
    "    return X, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3a3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1)   # single logit\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f1b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs=20, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trains the MLP using Adam optimizer and BCE With Logits Loss.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss() #loss=−[ylog(σ(z))+(1−y)log(1−σ(z))]\n",
    "\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(num_samples) #shuffles sample indices every epoch\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i in range(0, num_samples, batch_size): #mini batch training\n",
    "            indices = permutation[i:i + batch_size]\n",
    "\n",
    "            batch_X = X_train[indices]\n",
    "            batch_y = y_train[indices]\n",
    "\n",
    "            logits = model(batch_X).squeeze()\n",
    "            loss = loss_fn(logits, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6cf7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model and reports metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test).squeeze()\n",
    "        probabilities = torch.sigmoid(logits) #logits → probabilities → predictions\n",
    "        predictions = (probabilities >= 0.5).int().numpy()\n",
    "\n",
    "    y_true = y_test.numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, predictions, average=\"binary\"\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 0.4021\n",
      "Epoch 2/100 - Loss: 0.3881\n",
      "Epoch 3/100 - Loss: 0.3956\n",
      "Epoch 4/100 - Loss: 0.2964\n",
      "Epoch 5/100 - Loss: 0.4277\n",
      "Epoch 6/100 - Loss: 0.3074\n",
      "Epoch 7/100 - Loss: 0.2229\n",
      "Epoch 8/100 - Loss: 0.3772\n",
      "Epoch 9/100 - Loss: 0.2784\n",
      "Epoch 10/100 - Loss: 0.2756\n",
      "Epoch 11/100 - Loss: 0.4148\n",
      "Epoch 12/100 - Loss: 0.3598\n",
      "Epoch 13/100 - Loss: 0.2731\n",
      "Epoch 14/100 - Loss: 0.2223\n",
      "Epoch 15/100 - Loss: 0.2970\n",
      "Epoch 16/100 - Loss: 0.3030\n",
      "Epoch 17/100 - Loss: 0.2962\n",
      "Epoch 18/100 - Loss: 0.3139\n",
      "Epoch 19/100 - Loss: 0.3888\n",
      "Epoch 20/100 - Loss: 0.3855\n",
      "Epoch 21/100 - Loss: 0.2716\n",
      "Epoch 22/100 - Loss: 0.2353\n",
      "Epoch 23/100 - Loss: 0.2786\n",
      "Epoch 24/100 - Loss: 0.3867\n",
      "Epoch 25/100 - Loss: 0.3041\n",
      "Epoch 26/100 - Loss: 0.2688\n",
      "Epoch 27/100 - Loss: 0.1636\n",
      "Epoch 28/100 - Loss: 0.3043\n",
      "Epoch 29/100 - Loss: 0.3035\n",
      "Epoch 30/100 - Loss: 0.2656\n",
      "Epoch 31/100 - Loss: 0.2454\n",
      "Epoch 32/100 - Loss: 0.2031\n",
      "Epoch 33/100 - Loss: 0.2564\n",
      "Epoch 34/100 - Loss: 0.2406\n",
      "Epoch 35/100 - Loss: 0.2391\n",
      "Epoch 36/100 - Loss: 0.2205\n",
      "Epoch 37/100 - Loss: 0.2871\n",
      "Epoch 38/100 - Loss: 0.2539\n",
      "Epoch 39/100 - Loss: 0.2921\n",
      "Epoch 40/100 - Loss: 0.2908\n",
      "Epoch 41/100 - Loss: 0.2274\n",
      "Epoch 42/100 - Loss: 0.2032\n",
      "Epoch 43/100 - Loss: 0.4070\n",
      "Epoch 44/100 - Loss: 0.2089\n",
      "Epoch 45/100 - Loss: 0.2702\n",
      "Epoch 46/100 - Loss: 0.2115\n",
      "Epoch 47/100 - Loss: 0.2871\n",
      "Epoch 48/100 - Loss: 0.3149\n",
      "Epoch 49/100 - Loss: 0.3325\n",
      "Epoch 50/100 - Loss: 0.4029\n",
      "Epoch 51/100 - Loss: 0.2968\n",
      "Epoch 52/100 - Loss: 0.2093\n",
      "Epoch 53/100 - Loss: 0.2109\n",
      "Epoch 54/100 - Loss: 0.2885\n",
      "Epoch 55/100 - Loss: 0.2350\n",
      "Epoch 56/100 - Loss: 0.2410\n",
      "Epoch 57/100 - Loss: 0.1852\n",
      "Epoch 58/100 - Loss: 0.2246\n",
      "Epoch 59/100 - Loss: 0.2460\n",
      "Epoch 60/100 - Loss: 0.2743\n",
      "Epoch 61/100 - Loss: 0.1751\n",
      "Epoch 62/100 - Loss: 0.1907\n",
      "Epoch 63/100 - Loss: 0.2106\n",
      "Epoch 64/100 - Loss: 0.3130\n",
      "Epoch 65/100 - Loss: 0.3390\n",
      "Epoch 66/100 - Loss: 0.1736\n",
      "Epoch 67/100 - Loss: 0.2784\n",
      "Epoch 68/100 - Loss: 0.3238\n",
      "Epoch 69/100 - Loss: 0.2162\n",
      "Epoch 70/100 - Loss: 0.2528\n",
      "Epoch 71/100 - Loss: 0.3302\n",
      "Epoch 72/100 - Loss: 0.2185\n",
      "Epoch 73/100 - Loss: 0.1499\n",
      "Epoch 74/100 - Loss: 0.1801\n",
      "Epoch 75/100 - Loss: 0.2489\n",
      "Epoch 76/100 - Loss: 0.1788\n",
      "Epoch 77/100 - Loss: 0.1902\n",
      "Epoch 78/100 - Loss: 0.2353\n",
      "Epoch 79/100 - Loss: 0.1940\n",
      "Epoch 80/100 - Loss: 0.1943\n",
      "Epoch 81/100 - Loss: 0.2145\n",
      "Epoch 82/100 - Loss: 0.2553\n",
      "Epoch 83/100 - Loss: 0.2444\n",
      "Epoch 84/100 - Loss: 0.1814\n",
      "Epoch 85/100 - Loss: 0.1380\n",
      "Epoch 86/100 - Loss: 0.3731\n",
      "Epoch 87/100 - Loss: 0.2583\n",
      "Epoch 88/100 - Loss: 0.1873\n",
      "Epoch 89/100 - Loss: 0.3146\n",
      "Epoch 90/100 - Loss: 0.2151\n",
      "Epoch 91/100 - Loss: 0.2262\n",
      "Epoch 92/100 - Loss: 0.2310\n",
      "Epoch 93/100 - Loss: 0.1164\n",
      "Epoch 94/100 - Loss: 0.3202\n",
      "Epoch 95/100 - Loss: 0.1991\n",
      "Epoch 96/100 - Loss: 0.2377\n",
      "Epoch 97/100 - Loss: 0.1747\n",
      "Epoch 98/100 - Loss: 0.1643\n",
      "Epoch 99/100 - Loss: 0.3716\n",
      "Epoch 100/100 - Loss: 0.2401\n",
      "\n",
      "Evaluation Results:\n",
      "Accuracy : 0.825957235206365\n",
      "Precision: 0.6475195822454308\n",
      "Recall   : 0.6604527296937417\n",
      "F1-score : 0.6539222148978246\n",
      "Confusion Matrix:\n",
      " [[3991  540]\n",
      " [ 510  992]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = load_adult_dataset(\"/home/naman/Cryptonite-RTP-NamanGoel/Task-3/adult/adult_data.csv\")\n",
    "\n",
    "    # Preprocess\n",
    "    X, y = preprocess_features(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch tensors bec numpy array cannot track gradients, nor can interact with nn.Module\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    model = build_mlp(X_train_tensor.shape[1])\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        X_train_tensor,\n",
    "        y_train_tensor,\n",
    "        epochs=20,\n",
    "        batch_size=256,\n",
    "        lr=1e-3\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    acc, prec, rec, f1, cm = evaluate_model(\n",
    "        model, X_test_tensor, y_test_tensor\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"Accuracy :\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall   :\", rec)\n",
    "    print(\"F1-score :\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c2ddf",
   "metadata": {},
   "source": [
    "for epochs=20,  \n",
    "    batch_size=256,  \n",
    "    lr=1e-3  \n",
    "Epoch 1/20 - Loss: 0.3117  \n",
    "Epoch 2/20 - Loss: 0.2787  \n",
    "Epoch 3/20 - Loss: 0.4108  \n",
    "Epoch 4/20 - Loss: 0.2465  \n",
    "Epoch 5/20 - Loss: 0.3254  \n",
    "Epoch 6/20 - Loss: 0.4575  \n",
    "Epoch 7/20 - Loss: 0.3554  \n",
    "Epoch 8/20 - Loss: 0.2626  \n",
    "Epoch 9/20 - Loss: 0.2518  \n",
    "Epoch 10/20 - Loss: 0.2981  \n",
    "Epoch 11/20 - Loss: 0.3389  \n",
    "Epoch 12/20 - Loss: 0.3012  \n",
    "Epoch 13/20 - Loss: 0.4842  \n",
    "Epoch 14/20 - Loss: 0.3310  \n",
    "Epoch 15/20 - Loss: 0.3438  \n",
    "Epoch 16/20 - Loss: 0.2139  \n",
    "Epoch 17/20 - Loss: 0.2541  \n",
    "Epoch 18/20 - Loss: 0.2405  \n",
    "Epoch 19/20 - Loss: 0.3080  \n",
    "Epoch 20/20 - Loss: 0.2683  \n",
    "  \n",
    "Evaluation Results:  \n",
    "Accuracy : 0.8513177523620089  \n",
    "Precision: 0.7325134511913912  \n",
    "Recall   : 0.6344873501997337  \n",
    "F1-score : 0.6799857295754549  \n",
    "Confusion Matrix:  \n",
    " [[4183  348]  \n",
    " [ 549  953]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02678d20",
   "metadata": {},
   "source": [
    "Experiment A (20 epochs, batch size 256) is the better baseline model because it achieves a higher F1-score and maintains a stronger balance between precision and recall, while Experiment B (100 epochs, batch size 512) demonstrates the expected recall–precision trade-off when training longer with larger batches on an imbalanced dataset.  \n",
    "for epochs = 100 batch_size = 512 lr = 1e-3    \n",
    "Epoch 1/100 - Loss: 0.4021  \n",
    "Epoch 2/100 - Loss: 0.3881   \n",
    "Epoch 3/100 - Loss: 0.3956   \n",
    "Epoch 4/100 - Loss: 0.2964    \n",
    "Epoch 95/100 - Loss: 0.1991   \n",
    "Epoch 96/100 - Loss: 0.2377   \n",
    "Epoch 97/100 - Loss: 0.1747   \n",
    "Epoch 98/100 - Loss: 0.1643   \n",
    "Epoch 99/100 - Loss: 0.3716   \n",
    "Epoch 100/100 - Loss: 0.2401   \n",
    "Evaluation Results:   \n",
    "Accuracy : 0.825957235206365   \n",
    "Precision: 0.6475195822454308   \n",
    "Recall : 0.6604527296937417   \n",
    "F1-score : 0.6539222148978246   \n",
    "Confusion Matrix:   \n",
    "[[3991 540]   \n",
    "[ 510 992]  \n",
    "\n",
    "Thus, experiment B caught more individuals in >50k category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e4865",
   "metadata": {},
   "source": [
    "Methods  \n",
    "The Adult Income dataset was used to formulate a binary classification task predicting whether an individual earns more than $50K annually. The dataset contains both numerical and categorical features, which were preprocessed by standardizing numerical variables and applying one-hot encoding to categorical variables. A multi-layer perceptron (MLP) was implemented using PyTorch, with two hidden layers and ReLU activations. The model was trained using the Adam optimizer and binary cross-entropy loss with logits. Training was performed using mini-batch gradient descent, and model performance was evaluated using accuracy, precision, recall, F1-score, and a confusion matrix to account for class imbalance.  \n",
    "  \n",
    "Results  \n",
    "With 20 epochs and a batch size of 256, the model achieved an accuracy of 85.1%, a precision of 73.3%, a recall of 63.4%, and an F1-score of 0.68 for the high-income class. Increasing the training duration to 100 epochs and the batch size to 512 resulted in a higher recall (66.0%) but reduced precision (64.8%) and overall accuracy (82.6%), leading to a lower F1-score (0.65). These results illustrate the trade-off between precision and recall in the presence of class imbalance and establish the shorter training configuration as a stronger balanced baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptonite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
