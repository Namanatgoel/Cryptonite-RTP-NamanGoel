{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01a9a17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransformGetItemToIndex' from 'torch._higher_order_ops.flex_attention' (/home/naman/miniconda3/envs/cryptonite/lib/python3.10/site-packages/torch/_higher_order_ops/flex_attention.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# DEVICE SETUP\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/utils/import_utils.py:2044\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2043\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2044\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;66;03m# V5: If trying to import a *TokenizerFast symbol, transparently fall back to the\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;66;03m# non-Fast symbol from the same module when available. This lets us keep only one\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;66;03m# backend tokenizer class while preserving legacy public names.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/utils/import_utils.py:2238\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/utils/import_utils.py:2236\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2236\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2237\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cached_file\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     CONFIG_MAPPING_NAMES,\n\u001b[1;32m     40\u001b[0m     AutoConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     replace_list_option_in_docstrings,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:43\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[1;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     48\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/utils/import_utils.py:2044\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2043\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2044\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;66;03m# V5: If trying to import a *TokenizerFast symbol, transparently fall back to the\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;66;03m# non-Fast symbol from the same module when available. This lets us keep only one\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;66;03m# backend tokenizer class while preserving legacy public names.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/utils/import_utils.py:2238\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/utils/import_utils.py:2236\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2236\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2237\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/generation/utils.py:44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fsdp_managed_module\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasking_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_masks_for_generate\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_python\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExtensionsTrie\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     ModelOutput,\n\u001b[1;32m     48\u001b[0m     TransformersKwargs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     logging,\n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/transformers/masking_utils.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_torch_flex_attn_available, is_torch_greater_or_equal, is_tracing\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_flex_attn_available():\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DEFAULT_SPARSE_BLOCK_SIZE \u001b[38;5;28;01mas\u001b[39;00m flex_default_block_size\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockMask, create_block_mask\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Register a fake type to avoid crashing for annotations and `isinstance` checks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cryptonite/lib/python3.10/site-packages/torch/nn/attention/flex_attention.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     flex_attention \u001b[38;5;28;01mas\u001b[39;00m flex_attention_hop,\n\u001b[1;32m     18\u001b[0m     TransformGetItemToIndex,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _set_compilation_env\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproxy_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     _temp_remove_pre_dispatch_torch_function_mode,\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TransformGetItemToIndex' from 'torch._higher_order_ops.flex_attention' (/home/naman/miniconda3/envs/cryptonite/lib/python3.10/site-packages/torch/_higher_order_ops/flex_attention.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE SETUP\n",
    "# ============================================================\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODULE 1 — DATA INGESTION\n",
    "# ============================================================\n",
    "\n",
    "def load_raw_wikipedia(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    lines = text.splitlines()\n",
    "    cleaned = [line.strip() for line in lines if line.strip()]\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str, batch_size: int = 100_000) -> List[str]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"lemmatizer\"])\n",
    "    sentences = []\n",
    "\n",
    "    for i in range(0, len(text), batch_size):\n",
    "        slice_text = text[i:i + batch_size]\n",
    "        doc = nlp(slice_text)\n",
    "        for sent in doc.sents:\n",
    "            s = sent.text.strip()\n",
    "            if s:\n",
    "                sentences.append(s)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def ingest_wikipedia(path: str) -> List[str]:\n",
    "    raw = load_raw_wikipedia(path)\n",
    "    normalized = normalize_text(raw)\n",
    "    sentences = split_into_sentences(normalized)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODULE 2 — CHUNKING\n",
    "# ============================================================\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return int(len(text.split()) / 0.75)\n",
    "\n",
    "\n",
    "def build_chunks(\n",
    "    sentences: List[str],\n",
    "    min_tokens: int = 300,\n",
    "    max_tokens: int = 600,\n",
    "    overlap_tokens: int = 100\n",
    ") -> List[Dict]:\n",
    "\n",
    "    chunks = []\n",
    "    current_sentences = []\n",
    "    current_tokens = 0\n",
    "    chunk_id = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            if current_tokens >= min_tokens:\n",
    "                chunk_text = \" \".join(current_sentences)\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"token_count\": current_tokens\n",
    "                })\n",
    "                chunk_id += 1\n",
    "\n",
    "            overlap = []\n",
    "            overlap_count = 0\n",
    "            for sent in reversed(current_sentences):\n",
    "                t = count_tokens(sent)\n",
    "                if overlap_count + t > overlap_tokens:\n",
    "                    break\n",
    "                overlap.insert(0, sent)\n",
    "                overlap_count += t\n",
    "\n",
    "            current_sentences = overlap\n",
    "            current_tokens = overlap_count\n",
    "\n",
    "        current_sentences.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "    if current_tokens >= min_tokens:\n",
    "        chunks.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": \" \".join(current_sentences),\n",
    "            \"token_count\": current_tokens\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODULE 3 — RETRIEVAL (E5-LARGE + FAISS)\n",
    "# ============================================================\n",
    "\n",
    "def load_embedding_model():\n",
    "    model = SentenceTransformer(\"intfloat/e5-large\", device=DEVICE)\n",
    "    return model\n",
    "\n",
    "\n",
    "def embed_chunks(model, chunks: List[Dict], batch_size: int = 32):\n",
    "    texts = [\"passage: \" + c[\"text\"] for c in chunks]\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "def retrieve_top_k(\n",
    "    question: str,\n",
    "    model,\n",
    "    index,\n",
    "    chunks,\n",
    "    k: int = 3\n",
    "):\n",
    "\n",
    "    query_embedding = model.encode(\n",
    "        [\"query: \" + question],\n",
    "        normalize_embeddings=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    scores, ids = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(ids[0], scores[0]):\n",
    "        results.append((chunks[idx], float(score)))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODULE 4 — ANSWERER MODEL\n",
    "# ============================================================\n",
    "\n",
    "def load_answerer_model(model_name: str = \"distilgpt2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def build_prompt(question: str, retrieved_chunks: List[Dict]) -> str:\n",
    "    context = \"\\n\\n\".join(chunk[\"text\"] for chunk in retrieved_chunks)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an editor.\\n\"\n",
    "        \"Use ONLY the information in the given text.\\n\"\n",
    "        \"Do not add facts.\\n\"\n",
    "        \"If the text does not answer the question, say:\\n\"\n",
    "        \"\\\"Not enough information in the Simple Wikipedia dataset.\\\"\\n\"\n",
    "        \"Use simple English.\\n\"\n",
    "        \"Write at most 3 short sentences.\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        f\"Text:\\n{context}\\n\\n\"\n",
    "        \"Answer:\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_raw_answer(question, retrieved_chunks, tokenizer, model):\n",
    "\n",
    "    prompt = build_prompt(question, retrieved_chunks)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = decoded.split(\"Answer:\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODULE 5 — POST PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    return re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "\n",
    "\n",
    "def split_sentences_simple(text):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def merge_short_sentences(sentences, min_words=5):\n",
    "    merged = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for s in sentences:\n",
    "        if len(s.split()) < min_words:\n",
    "            buffer += \" \" + s\n",
    "        else:\n",
    "            if buffer:\n",
    "                merged.append((buffer + \" \" + s).strip())\n",
    "                buffer = \"\"\n",
    "            else:\n",
    "                merged.append(s)\n",
    "\n",
    "    if buffer:\n",
    "        merged.append(buffer.strip())\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def enforce_sentence_limit(sentences, max_sentences=3):\n",
    "    return sentences[:max_sentences]\n",
    "\n",
    "\n",
    "def check_refusal_needed(answer, retrieved_chunks, threshold=0.15):\n",
    "    retrieved_text = \" \".join(chunk[\"text\"] for chunk in retrieved_chunks)\n",
    "    retrieved_words = set(retrieved_text.lower().split())\n",
    "    answer_words = answer.lower().split()\n",
    "\n",
    "    unseen = [w for w in answer_words if w not in retrieved_words]\n",
    "\n",
    "    if len(unseen) / max(len(answer_words), 1) > threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def post_process_answer(raw_answer, retrieved_chunks):\n",
    "\n",
    "    cleaned = remove_parentheses(raw_answer)\n",
    "    sentences = split_sentences_simple(cleaned)\n",
    "    sentences = merge_short_sentences(sentences)\n",
    "    sentences = enforce_sentence_limit(sentences)\n",
    "\n",
    "    final_answer = \" \".join(sentences).strip()\n",
    "\n",
    "    if not final_answer:\n",
    "        return \"Not enough information in the Simple Wikipedia dataset.\"\n",
    "\n",
    "    if check_refusal_needed(final_answer, retrieved_chunks):\n",
    "        return \"Not enough information in the Simple Wikipedia dataset.\"\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODULE 6 — FULL PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "def answer_question(question, embedding_model, index, chunks, tokenizer, model):\n",
    "\n",
    "    retrieval_results = retrieve_top_k(\n",
    "        question,\n",
    "        embedding_model,\n",
    "        index,\n",
    "        chunks,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = [c for c, _ in retrieval_results]\n",
    "\n",
    "    raw_answer = generate_raw_answer(\n",
    "        question,\n",
    "        retrieved_chunks,\n",
    "        tokenizer,\n",
    "        model\n",
    "    )\n",
    "\n",
    "    final_answer = post_process_answer(\n",
    "        raw_answer,\n",
    "        retrieved_chunks\n",
    "    )\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    wiki_path = \"allcombined.txt\"\n",
    "\n",
    "    print(\"Loading & processing dataset...\")\n",
    "    sentences = ingest_wikipedia(wiki_path)\n",
    "    chunks = build_chunks(sentences)\n",
    "\n",
    "    print(\"Loading embedding model...\")\n",
    "    embedding_model = load_embedding_model()\n",
    "\n",
    "    print(\"Embedding chunks...\")\n",
    "    embeddings = embed_chunks(embedding_model, chunks)\n",
    "\n",
    "    print(\"Building FAISS index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    print(\"Loading answerer model...\")\n",
    "    tokenizer, answer_model = load_answerer_model(\"distilgpt2\")\n",
    "\n",
    "    while True:\n",
    "        q = input(\"\\nAsk a question (or type 'exit'): \")\n",
    "        if q.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        answer = answer_question(\n",
    "            q,\n",
    "            embedding_model,\n",
    "            index,\n",
    "            chunks,\n",
    "            tokenizer,\n",
    "            answer_model\n",
    "        )\n",
    "\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e403c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptonite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
